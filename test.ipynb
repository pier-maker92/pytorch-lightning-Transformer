{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_pl import TransformerPL\n",
    "from translate.storage.tmx import tmxfile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=\"lightning_logs/\",\n",
    "    filename=\"voiceformer-{epoch:02d}-{train_loss:.5f}\",\n",
    "    save_on_train_epoch_end = True\n",
    ")\n",
    "\n",
    "def EnIt_collate(batch): \n",
    "    src = []; tgt = []\n",
    "    for item in batch:\n",
    "        src.append(item[0])\n",
    "        tgt.append(item[1])\n",
    "    return torch.cat(src,dim=0), torch.cat(tgt,dim=0)\n",
    "\n",
    "class EnIt(Dataset):\n",
    "    def __init__(self, corpus_dir, split, split_val=.1, reduction=0):\n",
    "        with open(corpus_dir, 'rb') as fin:\n",
    "            tmx_file = tmxfile(fin, 'en', 'ar')\n",
    "\n",
    "        corpus = list(tmx_file.unit_iter())\n",
    "        if reduction:\n",
    "            corpus = corpus[:reduction]\n",
    "        \n",
    "        # split train/val     \n",
    "        if split == \"train\":\n",
    "            corpus = corpus[:int(len(corpus)*(1-split_val))]\n",
    "        else: \n",
    "            corpus = corpus[int(len(corpus)*(1-split_val)):]\n",
    "\n",
    "        self.tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        self.tokenizer_it = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-cased\")\n",
    "        \n",
    "        print('\\ncompute tokens ids for src')\n",
    "        src_tokens = [self.tokenizer_en(doc.source)['input_ids'] for doc in tqdm(corpus)]\n",
    "        print('compute tokens ids for tgt\\n')\n",
    "        tgt_tokens = [self.tokenizer_it(doc.target)['input_ids'] for doc in tqdm(corpus)]\n",
    "        # filter for samples with length <= 512 tokens\n",
    "        src_mask = np.where(np.array([512 <= len(tokens) for tokens in src_tokens]).astype(int)*-1+1)[0]\n",
    "        tgt_mask = np.where(np.array([512 <= len(tokens) for tokens in tgt_tokens]).astype(int)*-1+1)[0]\n",
    "        mask = np.array(list(set(src_mask) & set(tgt_mask)))\n",
    "        src_tokens = list(np.array(src_tokens)[mask])\n",
    "        tgt_tokens = list(np.array(tgt_tokens)[mask])\n",
    "        assert len(src_tokens) == len(tgt_tokens)\n",
    "        print(f\"{len(src_tokens)} samples in {split} set\")\n",
    "\n",
    "        self.corpus = {\n",
    "            'src': src_tokens,\n",
    "            'tgt': tgt_tokens\n",
    "        }\n",
    "        self.max_tgt_len = self.max_src_len = 512\n",
    "        self.pad_tgt_value = self.tokenizer_it.pad_token_id\n",
    "        self.pad_src_value = self.tokenizer_en.pad_token_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.corpus['src'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = torch.tensor(self.corpus['src'][idx]), torch.tensor(self.corpus['tgt'][idx])\n",
    "        # pad src\n",
    "        if src.size(-1) < self.max_src_len:\n",
    "            src = torch.nn.ConstantPad1d((0, self.max_src_len - src.size(-1)), self.pad_src_value)(src)\n",
    "        # eos tgt\n",
    "        if tgt.size(-1) < self.max_tgt_len:\n",
    "            tgt = torch.nn.ConstantPad1d((0, self.max_tgt_len - tgt.size(-1)), self.pad_tgt_value)(tgt)\n",
    "        \n",
    "        return src.unsqueeze(0), tgt.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "compute tokens ids for src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:00<00:00, 11259.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute tokens ids for tgt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:00<00:00, 10743.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 samples in train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/ld/tlpgfhsj05jb43v0ftcs76sh0000gp/T/ipykernel_20136/1741869486.py:57: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  src_tokens = list(np.array(src_tokens)[mask])\n",
      "/var/folders/ld/tlpgfhsj05jb43v0ftcs76sh0000gp/T/ipykernel_20136/1741869486.py:58: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tgt_tokens = list(np.array(tgt_tokens)[mask])\n"
     ]
    }
   ],
   "source": [
    "training_data = EnIt(corpus_dir=\"data/en-it.tmx\", split=\"train\", reduction=1000)\n",
    "train_dataloader = DataLoader(training_data, \n",
    "                                           batch_size = 4, \n",
    "                                           drop_last = True,\n",
    "                                           shuffle=True,\n",
    "                                           collate_fn=EnIt_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src,tgt = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2408,  8708,  ...,     0,     0,     0],\n",
       "        [  101,  5316,   119,  ...,     0,     0,     0],\n",
       "        [  101,  1109, 11336,  ...,     0,     0,     0],\n",
       "        [  101,  1109,  3442,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 102, 4445, 4445,  ...,    0,    0,    0],\n",
       "        [ 102, 2287,  697,  ...,    0,    0,    0],\n",
       "        [ 102,  329,  533,  ...,    0,    0,    0],\n",
       "        [ 102,  966, 3935,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.tokenizer_en.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
